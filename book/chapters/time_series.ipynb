{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "2d4dd90337994e43",
   "metadata": {
    "collapsed": false
   },
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
   "source": [
    "# Cumulative Time Series Analysis \n",
    "\n",
    "Importance of time series analysis in SWE data\n",
    "Methods and models for time series analysis\n"
<<<<<<< HEAD
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d4dd90337994e43"
=======
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6b2cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This chapter focus is on process and analyze Snow Water Equivalent (SWE) and meteorological data to prepare it for time series analysis. Addresses several key aspects of data preparation, including data cleaning, handling missing data, data interpolation, and transformation of data into a suitable format for analysis. The end goal is to facilitate the examination of SWE dynamics and related meteorological variables over time, enabling more accurate and insightful environmental and climatic analyses.\n",
    "\n",
    "## Program Execution Flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dea4f",
   "metadata": {},
   "source": [
    "### Importing the libraries and Input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f4a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"../data\"\n",
    "\n",
    "# Set Pandas options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# Define file paths for various CSV files\n",
    "current_ready_csv_path = f'{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv'\n",
    "cleaned_csv_path = f\"{current_ready_csv_path}_cleaned_nodata.csv\"\n",
    "target_time_series_csv_path = f'{cleaned_csv_path}_time_series_v1.csv'\n",
    "backup_time_series_csv_path = f'{cleaned_csv_path}_time_series_v1_bak.csv'\n",
    "target_time_series_cumulative_csv_path = f'{cleaned_csv_path}_time_series_cumulative_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9312996",
   "metadata": {},
   "source": [
    "Initially, it imports necessary libraries such as Pandas, os, shutil, numpy, and Dask.dataframe.\n",
    "\n",
    "**Pandas** is a powerful data manipulation library in Python used for data analysis and manipulation. It provides data structures like DataFrame and Series, which are highly efficient for handling structured data like CSV files\n",
    "\n",
    "The **shutil** module offers a high-level interface for file operations, including copying, moving, and removing files and directories. It's used for file manipulation tasks that are more complex than what the os module provides.\n",
    "\n",
    "**NumPy** is a fundamental library for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.\n",
    "\n",
    "**Dask** is a parallel computing library in Python that enables scalable and distributed computing. It extends the capabilities of libraries like Pandas, NumPy, and Scikit-learn to handle datasets that are larger than memory or can benefit from parallel processing. Dask dataframes, similar to Pandas dataframes, allow for efficient manipulation of larger-than-memory datasets by parallelizing operations across multiple cores or machines.\n",
    "\n",
    "By default, Pandas truncates the display of columns if they exceed a certain number, making it difficult to see the entire dataset when printed. So Pandas options are set to display all columns when printing data frames.\n",
    "\n",
    "We are importing the file which we have created in the previous chapter by integrating all the data from different sources.\n",
    "\n",
    "The defined file paths indicate paths for various stages of data processing, such as cleaned data, time-series data, and backup files. Overall, the code sets up file paths and options necessary for handling CSV data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f350a",
   "metadata": {},
   "source": [
    "### Cleaning and Preparation\n",
    "\n",
    "Cleaning the dataset to remove rows with missing SWE values, ensuring that the subsequent analysis is based on complete and accurate data. This step is critical for maintaining the integrity of the analysis, as missing values can significantly skew results and interpretations.\n",
    "\n",
    "#### clean_non_swe_rows\n",
    "\n",
    "- **Purpose:** Removes rows from the dataset where 'swe_value' is missing. This initial cleaning step is crucial for ensuring the dataset's integrity for further processing and analysis.\n",
    "- **Implementation:** Utilizes Dask for efficient processing of potentially large datasets, ensuring scalability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe491f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask_df_filtered.shape =  (Delayed('int-5cc8b7ae-25d5-478c-b53d-664926e9ce4f'), 26)\n",
      "The filtered csv with no swe values is saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path):\n",
    "    # Read Dask DataFrame from CSV\n",
    "    dask_df = dd.read_csv(current_ready_csv_path, dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "\n",
    "    # Remove rows where 'swe_value' is empty\n",
    "    dask_df_filtered = dask_df.dropna(subset=['swe_value'])\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    dask_df_filtered.to_csv(cleaned_csv_path, index=False, single_file=True)\n",
    "    print(\"dask_df_filtered.shape = \", dask_df_filtered.shape)\n",
    "    print(f\"The filtered csv with no swe values is saved to {cleaned_csv_path}\")\n",
    "\n",
    "clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b947383",
   "metadata": {},
   "source": [
    "### Interpolation and Missing Data Handling\n",
    "\n",
    "With a clean dataset, the script proceeds to interpolate missing values for specified columns. This step is essential for filling gaps in the dataset without introducing significant biases, allowing for a more continuous and comprehensive analysis of temporal trends.\n",
    "\n",
    "#### interpolate_missing_inplace\n",
    "\n",
    "- **Purpose:** Performs in-place interpolation of missing values for specified columns, using a polynomial of a given degree. Custom logic is applied based on the column name to appropriately handle different types of missing data.\n",
    "- **Details:** The function checks for specific conditions (e.g., SWE values above a certain threshold) to decide on the interpolation strategy. It ensures that no null values remain post-interpolation, maintaining the dataset's completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf8ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_inplace(df, column_name, degree=3):\n",
    "    x = df.index\n",
    "    y = df[column_name]\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    if column_name == \"SWE\":\n",
    "      mask = (y > 240) | y.isnull()\n",
    "    elif column_name == \"fsca\":\n",
    "      mask = (y > 100) | y.isnull()\n",
    "    else:\n",
    "      mask = y.isnull()\n",
    "\n",
    "    # Check if all elements in the mask array are True\n",
    "    all_true = np.all(mask)\n",
    "\n",
    "    if all_true:\n",
    "      df[column_name] = 0\n",
    "    else:\n",
    "      # Perform interpolation\n",
    "      new_y = np.interp(x, x[~mask], y[~mask])\n",
    "      # Replace missing values with interpolated values\n",
    "      df[column_name] = new_y\n",
    "\n",
    "    if np.any(df[column_name].isnull()):\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df06a6",
   "metadata": {},
   "source": [
    "This code snippet processes a DataFrame (df) by interpolating missing values in a specified column (column_name). First, it extracts the indices (x) and values (y) from the DataFrame for the specified column. Then, it creates a mask to identify missing values based on the column name. If the column is \"SWE\" or \"fsca\", values exceeding specific thresholds or being null are marked in the mask. Otherwise, any null values are marked. If all values in the mask are true, indicating all data points are missing, the column is filled with zeros. Otherwise, missing values are interpolated using the numpy interp function. After interpolation, it checks if there are any remaining null values in the column; if so, it raises a ValueError. Finally, it returns the processed DataFrame. Overall, this code ensures that missing values in the specified column are either filled with interpolated values or zeros, depending on the conditions specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55977c",
   "metadata": {},
   "source": [
    "### Conversion to Time Series Format\n",
    "\n",
    "Following interpolation, the script converts the dataset into a time series format. This transformation is pivotal for temporal analysis, enabling the examination of how SWE and meteorological variables change over time.\n",
    "\n",
    "#### convert_to_time_series\n",
    "\n",
    "- **Purpose:** Transforms the dataset into a time series format, sorting data and filling missing values with interpolated data. This step prepares the dataset for time series analysis, facilitating the examination of temporal patterns and trends.\n",
    "- **Process:** The function reads the cleaned CSV, sorts the data, performs interpolation for missing values, and structures the dataset for time series analysis. It handles specific columns identified for time series conversion, ensuring they are correctly processed and formatted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c537d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All current columns:  Index(['date', 'lat', 'lon', 'AMSR_SWE', 'station_name', 'swe_value',\n",
      "       'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n",
      "       'air_temperature_tmmn', 'potential_evapotranspiration',\n",
      "       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n",
      "       'relative_humidity_rmin', 'precipitation_amount',\n",
      "       'air_temperature_tmmx', 'wind_speed', 'stationTriplet', 'elevation',\n",
      "       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n",
      "       'fSCA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns_to_be_time_series = [\"SWE\", \n",
    "                                 'air_temperature_tmmn',\n",
    "                                 'potential_evapotranspiration', \n",
    "                                 'mean_vapor_pressure_deficit',\n",
    "                                 'relative_humidity_rmax', \n",
    "                                 'relative_humidity_rmin',\n",
    "                                 'precipitation_amount', \n",
    "                                 'air_temperature_tmmx', \n",
    "                                 'wind_speed',\n",
    "                                 'fsca']\n",
    "\n",
    "# Read the cleaned ready CSV\n",
    "df = pd.read_csv(cleaned_csv_path)\n",
    "df.sort_values(by=['lat', 'lon', 'date'], inplace=True)\n",
    "print(\"All current columns: \", df.columns)\n",
    "    \n",
    "    # rename all columns to unified names\n",
    "    #     ['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n",
    "# 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs',\n",
    "# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n",
    "# 'fsca']\n",
    "df.rename(columns={'vpd': 'mean_vapor_pressure_deficit',\n",
    "                        'vs': 'wind_speed', \n",
    "                        'pr': 'precipitation_amount', \n",
    "                        'etr': 'potential_evapotranspiration',\n",
    "                        'tmmn': 'air_temperature_tmmn',\n",
    "                        'tmmx': 'air_temperature_tmmx',\n",
    "                        'rmin': 'relative_humidity_rmin',\n",
    "                        'rmax': 'relative_humidity_rmax',\n",
    "                        'AMSR_SWE': 'SWE',\n",
    "                        'fSCA': 'fsca'\n",
    "                    }, inplace=True)\n",
    "\n",
    "filled_csv = f\"{target_time_series_csv_path}_gap_filled.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b71203",
   "metadata": {},
   "source": [
    "Above code snippet prepares a DataFrame for time series analysis by renaming selected columns to unified names and sorting the DataFrame based on latitude, longitude, and date. Initially, it specifies the columns to be considered as time series data. Then, it reads a cleaned CSV file into a Pandas DataFrame and sorts it based on latitude, longitude, and date. The code prints out all the current column names for reference. Next, it renames specific columns to standard names to ensure consistency across datasets. Finally, it defines a path for the output CSV file after gap filling, which is essential for completing the time series data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce41a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_v1.csv_gap_filled.csv already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(filled_csv):\n",
    "        print(f\"{filled_csv} already exists, skipping\")\n",
    "        filled_data = pd.read_csv(filled_csv)\n",
    "else:\n",
    "    # Function to perform polynomial interpolation and fill in missing values\n",
    "    def process_group_filling_value(group):\n",
    "        # Sort the group by 'date'\n",
    "        group = group.sort_values(by='date')\n",
    "    \n",
    "        for column_name in columns_to_be_time_series:\n",
    "            group = interpolate_missing_inplace(group, column_name)\n",
    "        # Return the processed group\n",
    "        return group\n",
    "    # Group the data by 'lat' and 'lon' and apply interpolation for each column\n",
    "    print(\"Start to fill in the missing values\")\n",
    "    grouped = df.groupby(['lat', 'lon'])\n",
    "    filled_data = grouped.apply(process_group_filling_value).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    if any(filled_data['fsca'] > 100):\n",
    "        raise ValueError(\"Error: shouldn't have SWE>240 at this point\")\n",
    "\n",
    "    filled_data.to_csv(filled_csv, index=False)\n",
    "    \n",
    "    print(f\"New filled values csv is saved to {filled_csv}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c9612",
   "metadata": {},
   "source": [
    "\n",
    "This code snippet checks if a CSV file named filled_csv exists. If it does, it prints a message indicating that the file exists and reads its contents into a Pandas DataFrame called filled_data. If the file doesn't exist, the code defines a function named process_group_filling_value to perform polynomial interpolation and fill missing values for each group sorted by date. Then, it groups the data by latitude and longitude, applies the interpolation function to fill missing values for each column specified in columns_to_be_time_series, and resets the index. After interpolation, it checks if any values in the 'fsca' column exceed 100; if so, it raises a ValueError. Finally, the filled DataFrame is saved to the CSV file filled_csv, and a message confirms the completion of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd07d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_v1.csv already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(target_time_series_csv_path):\n",
    "        print(f\"{target_time_series_csv_path} already exists, skipping\")\n",
    "else:\n",
    "    df = filled_data\n",
    "    # Create a new DataFrame to store the time series data for each location\n",
    "    print(\"Start to create the training csv with previous 7 days columns\")\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # Define the number of days to consider (7 days in this case)\n",
    "    num_days = 7\n",
    "\n",
    "    grouped = df.groupby(['lat', 'lon'])\n",
    "    \n",
    "    def process_group_time_series(group, num_days):\n",
    "        group = group.sort_values(by='date')\n",
    "        for day in range(1, num_days + 1):\n",
    "            for target_col in columns_to_be_time_series:\n",
    "                new_column_name = f'{target_col}_{day}'\n",
    "                group[new_column_name] = group[target_col].shift(day)\n",
    "            \n",
    "        return group\n",
    "    \n",
    "    result = grouped.apply(lambda group: process_group_time_series(group, num_days)).reset_index(drop=True)\n",
    "    result.fillna(0, inplace=True)\n",
    "    \n",
    "    result.to_csv(target_time_series_csv_path, index=False)\n",
    "    print(f\"New data is saved to {target_time_series_csv_path}\")\n",
    "    shutil.copy(target_time_series_csv_path, backup_time_series_csv_path)\n",
    "    print(f\"File is backed up to {backup_time_series_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff749c10",
   "metadata": {},
   "source": [
    "First checks if a CSV file named target_time_series_csv_path exists. If it does, it prints a message indicating that the file exists and skips further processing. If the file doesn't exist, it assigns the DataFrame filled_data to df and proceeds to create a new DataFrame named result to store time series data for each location. The code initializes the num_days variable to 7 and groups the data by latitude and longitude. It defines a function process_group_time_series to create columns for each target column shifted by the previous 7 days. These columns are then filled with corresponding values or zeros if no data is available. The result is saved to a CSV file target_time_series_csv_path, and a message confirms the completion of the process. Additionally, the newly created CSV file is backed up to backup_time_series_csv_path. In summary, this code generates a time series dataset by shifting data for specified columns over the previous 7 days and saves it to a CSV file, along with creating a backup copy of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117fa9",
   "metadata": {},
   "source": [
    "### Addition of Cumulative Columns\n",
    "\n",
    "To enrich the dataset further, the script adds cumulative columns for specific variables. This enhancement allows for the analysis of cumulative trends over time, providing additional insights into the accumulation and change of SWE and meteorological variables.\n",
    "\n",
    "#### add_cumulative_columns\n",
    "\n",
    "- **Purpose:** Calculates and adds cumulative values for specified variables to the dataset. This step enhances the dataset with cumulative information, facilitating more nuanced analyses of temporal trends and accumulations.\n",
    "- **Details:** The function reads the time series formatted dataset and computes cumulative values for each specified variable, saving the enhanced dataset for further analysis.\n",
    "\n",
    "### Execution and Output\n",
    "\n",
    "The script executes the defined functions in a structured sequence, starting from data cleaning, followed by interpolation, conversion to time series format, and finally, the addition of cumulative columns. The output is a thoroughly processed dataset ready for comprehensive time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b210f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def add_cumulative_columns(input_csv, output_csv, force=False):\n",
    "    \"\"\"\n",
    "    Add cumulative columns to the time series dataset.\n",
    "\n",
    "    This function reads the time series CSV file created by `convert_to_time_series`, calculates cumulative values for specific columns, and saves the data to a new CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_be_cumulated = [\n",
    "      \"SWE\",\n",
    "      'air_temperature_tmmn',\n",
    "      'potential_evapotranspiration', \n",
    "      'mean_vapor_pressure_deficit',\n",
    "      'relative_humidity_rmax', \n",
    "      'relative_humidity_rmin',\n",
    "      'precipitation_amount', \n",
    "      'air_temperature_tmmx', \n",
    "      'wind_speed',\n",
    "      'fsca'\n",
    "    ]\n",
    "\n",
    "    # Read the time series CSV (ensure it was created using `convert_to_time_series` function)\n",
    "    # directly read from original file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    print(\"The column statistics from time series before cumulative: \", df.describe()),\n",
    "   \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    unique_years = df['date'].dt.year.unique()\n",
    "    print(\"This is our unique years\", unique_years)\n",
    "    #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n",
    "    \n",
    "    # only start from the water year 10-01\n",
    "    # Filter rows based on the date range (2019 to 2022)\n",
    "    start_date = pd.to_datetime('2018-10-01')\n",
    "    end_date = pd.to_datetime('2021-09-30')\n",
    "    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    print(\"how many rows are left in the three water years?\", len(df.index))\n",
    "    df.to_csv(f\"{current_ready_csv_path}.test_check.csv\")\n",
    "\n",
    "    # Define a function to calculate the water year\n",
    "    def calculate_water_year(date):\n",
    "        year = date.year\n",
    "        if date.month >= 10:  # Water year starts in October\n",
    "            return year + 1\n",
    "        else:\n",
    "            return year\n",
    "    \n",
    "    # every water year starts at Oct 1, and ends at Sep 30. \n",
    "    df['water_year'] = df['date'].apply(calculate_water_year)\n",
    "    \n",
    "    # Group the DataFrame by 'lat' and 'lon'\n",
    "    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n",
    "    print(\"how many groups? \", len(grouped))\n",
    "    \n",
    "    # grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n",
    "    for column in columns_to_be_cumulated:\n",
    "        df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n",
    "\n",
    "    print(\"This is the dataframe after cumulative columns are added\")\n",
    "    print(df.columns)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"All the cumulative variables are added successfully! {target_time_series_cumulative_csv_path}\")\n",
    "    print(\"double check the swe_value statistics:\", df[\"swe_value\"].describe())\n",
    "    cumulative_columns = [f'cumulative_{col}' for col in columns_to_be_cumulated]\n",
    "    # Number of rows and columns for subplots\n",
    "    n_rows = len(cumulative_columns) // 2\n",
    "    n_cols = 2\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
    "\n",
    "    # Flatten axes array if more than one row\n",
    "    if n_rows > 1:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    first_group_name = list(grouped.groups.keys())[0]\n",
    "\n",
    "    # Get the first group\n",
    "    first_group = grouped.get_group(first_group_name)\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print(first_group) \n",
    "\n",
    "    # Plot each cumulative column in a subplot\n",
    "    for i, col in enumerate(cumulative_columns):\n",
    "        axes[i].plot(first_group['date'], first_group[col], label=col)\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].set_xlabel('Date')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Adjust layout for readability\n",
    "    plt.suptitle(f'Cumulative values of features for location: {first_group_name}', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.88)  # Adjust the top of the subplot box to 88% of the figure height\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "add_cumulative_columns(target_time_series_csv_path, target_time_series_cumulative_csv_path, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e6926",
   "metadata": {},
   "source": [
    "![image](../img/timeseries/cumulative-timeseries-graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf9bbf",
   "metadata": {},
   "source": [
    "The addition of cumulative columns to the dataset offers a dynamic perspective on environmental time series data. By aggregating measurements such as temperature, precipitation, and wind speed, the function reveals the incremental changes and long-term accumulation of these factors over specific periods – particularly water years. This comprehensive view is instrumental in understanding trends and patterns that are not immediately apparent in daily or monthly data.\n",
    "\n",
    "Cumulative statistics are particularly useful for examining the progressive impact of climatic variables on ecological systems. For instance, the cumulative snow-water equivalent (SWE) can indicate potential water resource trends, while cumulative evapotranspiration might reflect on agricultural or hydrological cycles. By tracking these accumulations over time and across different geographic locations, the function enables researchers and analysts to discern gradual environmental changes, assess resource availability, and predict ecological responses to climatic variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8637fdb",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "The script effectively prepares SWE and meteorological data for detailed time series analysis by systematically cleaning the data, handling missing values through interpolation, transforming the data into a time series format, and enriching the dataset with cumulative information. This preparation allows for a more accurate and insightful exploration of environmental and climatic dynamics."
   ]
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
<<<<<<< HEAD
    "version": 2
=======
    "version": 3
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
=======
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
